{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c411caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import keras \n",
    "import rawpy\n",
    "import random\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Embedding,Dropout,Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(11)\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import keras \n",
    "import rawpy\n",
    "import random\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Embedding,Dropout,Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(11)\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b20653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_path = \"./Training\"\n",
    "train_path_fire = train_path+ \"/Fire\"\n",
    "train_path_no_fire = train_path+ \"/No_Fire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8eeb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('pdf')\n",
    "\n",
    "METRICS = ['accuracy',\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')]\n",
    "\n",
    "def createModel(baseModel, hidden_layers, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Transfer Learning\n",
    "    model.add(baseModel)\n",
    "    model.layers[0].trainable = True  \n",
    "    model.add(Conv2DTranspose(16, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2DTranspose(32, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    model.add(Conv2DTranspose(64, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(UpSampling2D((2, 2)))   \n",
    "    model.add(Conv2DTranspose(128, kernel_size=(3,3), activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Hidden Layers\n",
    "    for layer in range(len(hidden_layers)):\n",
    "        model.add(Dense(hidden_layers[layer], activation='relu'))\n",
    "\n",
    "    # Add our classification layer and display model properties\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Compile the sections into one NN\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "                  metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "\n",
    "def trainModel(datasetPath, model, epochs, batch_size, image_size, preprocess_input):\n",
    "    data_generator = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input)\n",
    "\n",
    "    train_generator = data_generator.flow_from_directory(\n",
    "        f'{datasetPath}',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    num_training_files = len(train_generator.filepaths)\n",
    "    \n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
    "    lrr= ReduceLROnPlateau(monitor='accuracy',   factor=.1,   patience=2,  min_lr=1e-5) \n",
    "\n",
    "    # The division by 5 here is to make training quicker\n",
    "    # when you want maximum accuracy remove it.\n",
    "    return model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[lrr,es_callback])\n",
    "\n",
    "\n",
    "def create_pdf(history, model_name):\n",
    "    print(\"Generating pdf for \" + model_name)\n",
    "\n",
    "    # grab the desired metrics from tf history\n",
    "    desired_metrics = ['accuracy', 'loss', 'val_accuracy', 'val_loss']\n",
    "    metrics_to_plot = dict((k, history.history[k]) for k in desired_metrics if k in history.history)\n",
    "\n",
    "    df = pd.DataFrame(metrics_to_plot)\n",
    "    df.index = range(1, len(df)+1)\n",
    "    df.rename(columns={'accuracy': 'Training Accuracy', 'loss': 'Training Loss',\n",
    "        'val_accuracy': 'Validation Accuracy', 'val_loss': 'Validation Loss'}, inplace=True)\n",
    "    print(df)\n",
    "    sns.set()\n",
    "    ax = sns.lineplot(marker='o', dashes=False, data=df)\n",
    "    ax.set_xticks([1, 2, 3, 4, 5])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Model Loss and Accuracy')\n",
    "    plt.savefig(f'./model_statistics/{model_name}_plot.pdf')\n",
    "\n",
    "    print(\"Done\")\n",
    "    \n",
    "def testModel(model, batch_size, datasetPath, num_classes, model_name, image_size, preprocess_input, output_statistics=True):\n",
    "    data_generator = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input)\n",
    "\n",
    "    print(\"Test\" + model_name)\n",
    "    test_generator = data_generator.flow_from_directory(\n",
    "        f'./Test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    num_files = len(test_generator.filepaths)\n",
    "\n",
    "    steps = num_files/batch_size\n",
    "\n",
    "    model.evaluate(test_generator, steps=steps)\n",
    "    if(output_statistics):\n",
    "        generateStatistics(model, test_generator, model_name, num_classes, steps, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2fa4a4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_15 (Functional)       (None, 7, 7, 1280)        2257984   \n",
      "                                                                 \n",
      " conv2d_transpose_46 (Conv2D  (None, 9, 9, 16)         184336    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_47 (Conv2D  (None, 11, 11, 32)       4640      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_18 (UpSamplin  (None, 22, 22, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_48 (Conv2D  (None, 24, 24, 64)       18496     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_19 (UpSamplin  (None, 48, 48, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_49 (Conv2D  (None, 50, 50, 128)      73856     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 50, 50, 1)         1153      \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 128)               320128    \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,870,995\n",
      "Trainable params: 2,836,883\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Found 39375 images belonging to 2 classes.\n",
      "616/616 [==============================] - 360s 577ms/step - loss: 0.0425 - accuracy: 0.9825 - precision: 0.9825 - recall: 0.9825 - lr: 0.0100\n",
      "Generating pdf for efficientnetb7\n",
      "   Training Accuracy  Training Loss\n",
      "1           0.982451       0.042466\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import lib\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "dataset = \"./Training\"\n",
    "output_pdf = True\n",
    "output_statistics = False\n",
    "image_size = 224\n",
    "model_name = 'efficientnetb7'\n",
    "hidden_layers = [128, 64, 32]\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# Multiclass Settings\n",
    "# dataset = '/storage/deepfire/subsampledDatasets/forest-1-smoke-fire-forest' # Name of the folder in /storage/deepfire/subsampledDatasets\n",
    "# num_classes = 3\n",
    "\n",
    "\n",
    "baseModel = tf.keras.applications.MobileNetV2(\n",
    "    alpha=1.0,\n",
    "    input_shape= (224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg'\n",
    ")\n",
    "#baseModel.summary()\n",
    "new_model = tf.keras.Model(baseModel.input, baseModel.get_layer(name = \"out_relu\").output )\n",
    "#new_model.summary()\n",
    "\n",
    "# baseModel = EfficientNetB7(\n",
    "#     include_top=False, pooling='avg', weights='imagenet')\n",
    "\n",
    "fire_detector_model = createModel(\n",
    "    new_model, hidden_layers, num_classes)\n",
    "\n",
    "history = trainModel(dataset, fire_detector_model,\n",
    "                         epochs, batch_size, image_size, preprocess_input)\n",
    "create_pdf(history, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0df87d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pdf for efficientnetb7\n",
      "   Training Accuracy  Training Loss\n",
      "1           0.982451       0.042466\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_pdf(history, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fb9e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testefficientnetb7\n",
      "Found 8617 images belonging to 2 classes.\n",
      "134/134 [==============================] - 20s 139ms/step - loss: 1.5979 - accuracy: 0.6875 - precision: 0.6875 - recall: 0.6875\n"
     ]
    }
   ],
   "source": [
    "testModel(fire_detector_model, batch_size, dataset,\n",
    "              num_classes, model_name, image_size, preprocess_input, output_statistics)\n",
    "#fire_detector_model.save(f'saved_models/{model_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab6fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bb9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+-----+------+\n",
      "|origin                                                                                            |width|height|\n",
      "+--------------------------------------------------------------------------------------------------+-----+------+\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8363.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8367.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8368.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8366.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8412.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8359.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8369.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame5430.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8409.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8407.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8357.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8402.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8411.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8403.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8355.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8360.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8408.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8356.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8365.jpg|254  |254   |\n",
      "|file:///E:/Facultate/Siva_master/Projects/git/change_detection/Training/Fire/resized_frame8364.jpg|254  |254   |\n",
      "+--------------------------------------------------------------------------------------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Protob Conversion to Parquet\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(train_path_fire)\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea723e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_model = tf.keras.applications.EfficientNetV2B1(\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(45,64,4),\n",
    "    pooling=None,\n",
    "    include_preprocessing=True,\n",
    ")\n",
    "\n",
    "\n",
    "prev_model = tf.keras.applications.MobileNet(\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(45,64,4),\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(prev_model)\n",
    "#prev_model.summary()\n",
    "model.add(Reshape((4,250,)))\n",
    "#model.add(Reshape((1,1000,)))\n",
    "#model.add(Flatten)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d677a7",
   "metadata": {},
   "source": [
    "# ./FIRE-SMOKE-DATASET/FIRE-SMOKE-DATASET/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc1f94d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        (None, 7, 7, 1280)        2257984   \n",
      "                                                                 \n",
      " conv2d_transpose_8 (Conv2DT  (None, 9, 9, 16)         184336    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_9 (Conv2DT  (None, 11, 11, 32)       4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 22, 22, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_10 (Conv2D  (None, 24, 24, 64)       18496     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 48, 48, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_11 (Conv2D  (None, 50, 50, 128)      73856     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 50, 50, 1)         1153      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               320128    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,871,028\n",
      "Trainable params: 2,836,916\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Found 2700 images belonging to 3 classes.\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - 29s 571ms/step - loss: 1.1010 - accuracy: 0.3481 - precision: 0.2600 - recall: 0.0024 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 25s 587ms/step - loss: 1.0470 - accuracy: 0.4048 - precision: 0.6552 - recall: 0.0493 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 25s 584ms/step - loss: 0.6569 - accuracy: 0.7611 - precision: 0.8243 - recall: 0.5996 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 26s 590ms/step - loss: 0.2133 - accuracy: 0.9415 - precision: 0.9478 - recall: 0.9341 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 25s 584ms/step - loss: 0.0864 - accuracy: 0.9763 - precision: 0.9770 - recall: 0.9752 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 25s 574ms/step - loss: 0.0451 - accuracy: 0.9885 - precision: 0.9889 - recall: 0.9881 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 25s 568ms/step - loss: 0.0316 - accuracy: 0.9922 - precision: 0.9922 - recall: 0.9922 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 25s 573ms/step - loss: 0.0156 - accuracy: 0.9981 - precision: 0.9981 - recall: 0.9974 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 25s 577ms/step - loss: 0.0125 - accuracy: 0.9970 - precision: 0.9970 - recall: 0.9970 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 25s 575ms/step - loss: 0.0161 - accuracy: 0.9959 - precision: 0.9963 - recall: 0.9959 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 24s 563ms/step - loss: 0.0098 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9967 - lr: 1.0000e-03\n",
      "Generating pdf for efficientnetb7\n",
      "    Training Accuracy  Training Loss\n",
      "1            0.348148       1.101004\n",
      "2            0.404815       1.046957\n",
      "3            0.761111       0.656853\n",
      "4            0.941481       0.213265\n",
      "5            0.976296       0.086385\n",
      "6            0.988519       0.045146\n",
      "7            0.992222       0.031640\n",
      "8            0.998148       0.015578\n",
      "9            0.997037       0.012457\n",
      "10           0.995926       0.016108\n",
      "11           0.996667       0.009782\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import lib\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "dataset = \"./FIRE-SMOKE-DATASET/FIRE-SMOKE-DATASET/Train\"\n",
    "output_pdf = True\n",
    "output_statistics = False\n",
    "image_size = 224\n",
    "model_name = 'efficientnetb7'\n",
    "hidden_layers = [128, 64, 32]\n",
    "num_classes = 3\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# Multiclass Settings\n",
    "# dataset = '/storage/deepfire/subsampledDatasets/forest-1-smoke-fire-forest' # Name of the folder in /storage/deepfire/subsampledDatasets\n",
    "# num_classes = 3\n",
    "\n",
    "\n",
    "baseModel = tf.keras.applications.MobileNetV2(\n",
    "    alpha=1.0,\n",
    "    input_shape= (224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg'\n",
    ")\n",
    "#baseModel.summary()\n",
    "new_model = tf.keras.Model(baseModel.input, baseModel.get_layer(name = \"out_relu\").output )\n",
    "#new_model.summary()\n",
    "\n",
    "# baseModel = EfficientNetB7(\n",
    "#     include_top=False, pooling='avg', weights='imagenet')\n",
    "\n",
    "fire_detector_model = createModel(\n",
    "    new_model, hidden_layers, num_classes)\n",
    "\n",
    "history = trainModel(dataset, fire_detector_model,\n",
    "                         epochs, batch_size, image_size, preprocess_input)\n",
    "create_pdf(history, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949278a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testefficientnetb7\n",
      "Found 300 images belonging to 3 classes.\n",
      "4/4 [==============================] - 3s 516ms/step - loss: 0.8962 - accuracy: 0.7833 - precision: 0.7912 - recall: 0.7833\n"
     ]
    }
   ],
   "source": [
    "def testModel(model, batch_size, datasetPath, num_classes, model_name, image_size, preprocess_input, output_statistics=True):\n",
    "    data_generator = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input)\n",
    "    print(\"Test\" + model_name)\n",
    "    test_generator = data_generator.flow_from_directory(\n",
    "        f'FIRE-SMOKE-DATASET/FIRE-SMOKE-DATASET/Test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    num_files = len(test_generator.filepaths)\n",
    "\n",
    "    steps = num_files/batch_size\n",
    "\n",
    "    model.evaluate(test_generator, steps=steps)\n",
    "    if(output_statistics):\n",
    "        generateStatistics(model, test_generator, model_name, num_classes, steps, loss, accuracy)\n",
    "\n",
    "testModel(fire_detector_model, batch_size, dataset,\n",
    "              num_classes, model_name, image_size, preprocess_input, output_statistics)\n",
    "#fire_detector_model.save(f'saved_models/{model_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a79366",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_detector_model.save(f'saved_models/efficientnet_b7_FIRE-SMOKE-DATASET.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e8bb6",
   "metadata": {},
   "source": [
    "# E:\\Facultate\\Siva_master\\Projects\\git\\change_detection\\fire_smoke\\Dataset\\Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98835877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_6 (Functional)        (None, 7, 7, 1280)        2257984   \n",
      "                                                                 \n",
      " conv2d_transpose_24 (Conv2D  (None, 9, 9, 16)         184336    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_25 (Conv2D  (None, 11, 11, 32)       4640      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 22, 22, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_26 (Conv2D  (None, 24, 24, 64)       18496     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 48, 48, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_27 (Conv2D  (None, 50, 50, 128)      73856     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 50, 50, 1)         1153      \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2500)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               320128    \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,870,995\n",
      "Trainable params: 2,836,883\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import lib\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "dataset = \"./fire_smoke/Dataset/Training and Validation\"\n",
    "output_pdf = True\n",
    "output_statistics = False\n",
    "image_size = 224\n",
    "model_name = 'efficientnetb7'\n",
    "hidden_layers = [128, 64, 32]\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "# Multiclass Settings\n",
    "# dataset = '/storage/deepfire/subsampledDatasets/forest-1-smoke-fire-forest' # Name of the folder in /storage/deepfire/subsampledDatasets\n",
    "# num_classes = 3\n",
    "\n",
    "\n",
    "baseModel = tf.keras.applications.MobileNetV2(\n",
    "    alpha=1.0,\n",
    "    input_shape= (224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg'\n",
    ")\n",
    "#baseModel.summary()\n",
    "new_model = tf.keras.Model(baseModel.input, baseModel.get_layer(name = \"out_relu\").output )\n",
    "#new_model.summary()\n",
    "\n",
    "# baseModel = EfficientNetB7(\n",
    "#     include_top=False, pooling='avg', weights='imagenet')\n",
    "\n",
    "fire_detector_model = createModel(\n",
    "    new_model, hidden_layers, num_classes)\n",
    "\n",
    "# history = trainModel(dataset, fire_detector_model,\n",
    "#                          epochs, batch_size, image_size, preprocess_input)\n",
    "# create_pdf(history, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e64249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1520 images belonging to 2 classes.\n",
      "24/24 [==============================] - 14s 571ms/step - loss: 0.0028 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - lr: 0.0100\n",
      "Testefficientnetb7\n",
      "Found 380 images belonging to 2 classes.\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1885 - accuracy: 0.9368 - precision: 0.9368 - recall: 0.9368\n"
     ]
    }
   ],
   "source": [
    "def testModel(model, batch_size, datasetPath, num_classes, model_name, image_size, preprocess_input, output_statistics=True):\n",
    "    data_generator = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input)\n",
    "    print(\"Test\" + model_name)\n",
    "    test_generator = data_generator.flow_from_directory(\n",
    "        f'fire_smoke/Dataset/Testing',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    num_files = len(test_generator.filepaths)\n",
    "\n",
    "    steps = num_files/batch_size\n",
    "\n",
    "    model.evaluate(test_generator, steps=steps)\n",
    "    if(output_statistics):\n",
    "        generateStatistics(model, test_generator, model_name, num_classes, steps, loss, accuracy)\n",
    "\n",
    "dataset = \"./fire_smoke/Dataset/Training and Validation\"\n",
    "history = trainModel(dataset, fire_detector_model,\n",
    "                         1, batch_size, image_size, preprocess_input)\n",
    "\n",
    "testModel(fire_detector_model, batch_size, dataset,\n",
    "              num_classes, model_name, image_size, preprocess_input, output_statistics)\n",
    "#fire_detector_model.save(f'saved_models/{model_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1446d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_detector_model.save(f'saved_models/fire_smoke-Dataset-esting_94_acc.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf60eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39375 images belonging to 2 classes.\n",
      "616/616 [==============================] - 1897s 3s/step - loss: 0.0229 - accuracy: 0.9926 - precision: 0.9926 - recall: 0.9926 - lr: 0.0100\n",
      "Testefficientnetb7\n",
      "Found 8617 images belonging to 2 classes.\n",
      "134/134 [==============================] - 18s 133ms/step - loss: 0.8137 - accuracy: 0.6926 - precision: 0.6926 - recall: 0.6926\n"
     ]
    }
   ],
   "source": [
    "def testModel(model, batch_size, datasetPath, num_classes, model_name, image_size, preprocess_input, output_statistics=True):\n",
    "    data_generator = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input)\n",
    "\n",
    "    print(\"Test\" + model_name)\n",
    "    test_generator = data_generator.flow_from_directory(\n",
    "        f'./Test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "    num_files = len(test_generator.filepaths)\n",
    "\n",
    "    steps = num_files/batch_size\n",
    "\n",
    "    model.evaluate(test_generator, steps=steps)\n",
    "    if(output_statistics):\n",
    "        generateStatistics(model, test_generator, model_name, num_classes, steps, loss, accuracy)\n",
    "\n",
    "dataset = \"./Training\"\n",
    "output_pdf = True\n",
    "output_statistics = False\n",
    "image_size = 224\n",
    "model_name = 'efficientnetb7'\n",
    "hidden_layers = [128, 64, 32]\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "\n",
    "history = trainModel(dataset, fire_detector_model,\n",
    "                         1, batch_size, image_size, preprocess_input)\n",
    "\n",
    "testModel(fire_detector_model, batch_size, dataset,\n",
    "              num_classes, model_name, image_size, preprocess_input, output_statistics)\n",
    "#fire_detector_model.save(f'saved_models/{model_name}.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4688e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
